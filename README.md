# Data Project Template

<h1 align="center">
A Template project for Data Science & AI Projects
</h1>

## Cookiecutter Data Science
This project template is a simplified version of the [Cookiecutter Data Science](https://cookiecutter-data-science.drivendata.org) template, created to suit the needs of Datalumina and made available as a GitHub template.

## Project Setup

### 1. UV Installation
This project uses uv for dependency management. If you haven't installed uv yet, you can do so by following the [official installation guide](https://docs.astral.sh/uv/getting-started/installation/).

### 2. Setting up the Project
Clone the repository and navigate to the project directory:

> Make sure the host machine has `python = ">=3.10,<3.13"`

```bash
uv venv my-env
source .my-env/bin/activate
uv pip install -r pyproject.toml

```

### 3. Run the Jupyter Lab serverðŸš€

After Environment is activated, run below command for Jupyter lab Server

```bash

Jupyter lab

```

## Adjusting .gitignore

Ensure you adjust the `.gitignore` file according to your project needs. For example, since this is a template, the `/data/` folder is commented out and data will not be exlucded from source control:

```plaintext
# exclude data from source control by default
# /data/
```

Typically, you want to exclude this folder if it contains either sensitive data that you do not want to add to version control or large files.

## Duplicating the .env File
To set up your environment variables, you need to duplicate the `.env.example` file and rename it to `.env`. You can do this manually or using the following terminal command:

```bash
cp .env.example .env # Linux, macOS, Git Bash, WSL
copy .env.example .env # Windows Command Prompt
```

This command creates a copy of `.env.example` and names it `.env`, allowing you to configure your environment variables specific to your setup.


## Project Organization

```
â”œâ”€â”€ LICENSE            <- Open-source license if one is chosen
â”œâ”€â”€ README.md          <- The top-level README for developers using this project
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ external       <- Data from third party sources
â”‚   â”œâ”€â”€ interim        <- Intermediate data that has been transformed
â”‚   â”œâ”€â”€ processed      <- The final, canonical data sets for modeling
â”‚   â””â”€â”€ raw            <- The original, immutable data dump
â”‚
â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
â”‚
â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
â”‚                         the creator's initials, and a short `-` delimited description, e.g.
â”‚                         `1.0-jqp-initial-data-exploration`
â”‚
â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials
â”‚
â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
â”‚   â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
â”‚
â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
â”‚                         generated with `pip freeze > requirements.txt`
â”‚
â””â”€â”€ src                         <- Source code for this project
    â”‚
    â”œâ”€â”€ __init__.py             <- Makes src a Python module
    â”‚
    â”œâ”€â”€ config.py               <- Store useful variables and configuration
    â”‚
    â”œâ”€â”€ dataset.py              <- Scripts to download or generate data
    â”‚
    â”œâ”€â”€ features.py             <- Code to create features for modeling
    â”‚
    â”‚    
    â”œâ”€â”€ modeling                
    â”‚   â”œâ”€â”€ __init__.py 
    â”‚   â”œâ”€â”€ predict.py          <- Code to run model inference with trained models          
    â”‚   â””â”€â”€ train.py            <- Code to train models
    â”‚
    â”œâ”€â”€ plots.py                <- Code to create visualizations 
    â”‚
    â””â”€â”€ services                <- Service classes to connect with external platforms, tools, or APIs
        â””â”€â”€ __init__.py 
```

--------